# Session Log — ANIMUS

*Append only. Do not edit existing entries.*

---

## Entry #0 — 2026-01-26 01:08:53

### Summary
Project initialized. GECK structure created.

### Understood Goals
- Goal: Create a high-performance, cross-platform (Linux/macOS/Windows/Jetson) CLI coding agent. Core Philosophy:
- Local-First: Prioritize local inference (Ollama, TensorRT-LLM) but support APIs.
- Universal Ingestion: "Read" everything (Code, PDFs, ZIM archives) via RAG/Indexing, not fine-tuning.
- Orchestration: Capable of spawning sub-agents for specialized tasks.
- Hardware Aware: Optimized for edge hardware (Jetson Orin Nano) and standard desktops.
- Architecture & Implementation Phases
- Phase 1: The Core Shell (Skeleton)
- Objective: Build the CLI entry point and environment detection.
- Action: Initialize a Typer app structure.
- Action: Implement animus detect command to identify OS (Win/Mac/Linux) and Hardware (Jetson/x86/Apple Silicon).
- Action: Create the Configuration Manager (~/.animus/config.yaml) to store preferred model provider and paths.
- Constraint: Ensure all file paths use pathlib for Windows compatibility.
- Phase 2: The "Brain" Socket (Model Layer)
- Objective: Create a unified interface for model inference.
- Action: Create a ModelProvider abstract base class.
- Action: Implement OllamaProvider (connects to localhost:11434).
- Action: Implement TRTLLMProvider (Specific for Jetson: loads engine files).
- Action: Implement APIProvider (Standard HTTP requests).
- Key Logic: On boot, check if the configured model is available. If not, prompt the user to run animus pull <model>.
- Default Model: "Josified Qwen2.5-Coder" (or similar 7B coding model).
- Phase 3: The Librarian (RAG & Ingestion)
- Objective: Ingest massive datasets without crashing RAM.
- Action: Implement animus ingest <path> command.
- Strategy:
- Scanner: Walk directory tree respecting .gitignore.
- Router: Identify file type (ext).
- Extractor:
- ZIM/PDF: Stream read. Do not load full file into RAM. Chunk text into 512-token blocks.
- Code: Parse using Tree-sitter to preserve function scope.
- Indexer: Generate embeddings and upsert to Vector DB.
- Constraint: Implement a progress bar (Rich) during ingestion.
- Phase 4: The Agentic Loop (Reasoning Engine)
- Objective: The "Thought -> Act -> Observe" cycle.
- Action: Implement the Agent class.
- Tools:
- read_file(path)
- write_file(path, content)
- list_dir(path)
- run_shell(command) -> Must include Human-in-the-Loop confirmation for destructive commands (rm, git push).
- Context: Manage a sliding context window. Always keep the System Prompt + Last
- N
- turns + Retrieved RAG data.
- Phase 5: The Hive (Sub-Agent Orchestration)
- Objective: Handle complex tasks via delegation.
- Action: Implement a spawn_subagent(task_description, specialized_role) function.
- Logic:
- Main Agent realizes a task is too broad (e.g., "Refactor this entire module").
- Main Agent defines scope and constraints.
- Sub-Agent initializes with a restricted context (only relevant files).
- Sub-Agent reports back upon completion or failure.

### Questions/Ambiguities
None

### Initial Tasks
- Build the CLI entry point and environment detection.
- All commands execute without errors
- Help text is accurate and complete
- Exit codes are correct
- Input validation works properly
- Error messages are clear and actionable
- LLM Agent can execute commands via the terminal
- LLM Agent can create code (in various languages like Bash, Python, C++, Rust, Javascript, CSS, etc...) that functions and does so according to the original instructions
- LLM Agent can create specialized sub-agents, based in part off of Claude Codes sub-agent structure (View Claude Code Docmentation)

### Checkpoint
**Status:** WAIT — Awaiting confirmation to begin work.

---

## Entry #1 — 2026-01-26

### Summary
Completed Phase 1: Core Shell implementation. Built CLI entry point with Typer, environment detection (OS/hardware/GPU), and configuration management.

### Actions
- Created project directory structure using `src/` (renamed from `animus/` per user request to avoid confusion)
- Initialized `pyproject.toml` with dependencies (Typer, Rich, PyYAML, Pydantic)
- Implemented `src/core/detection.py` with OS, architecture, and hardware type detection
- Implemented `src/core/config.py` with ConfigManager and YAML-based configuration
- Created `src/main.py` as CLI entry point with Typer
- Implemented commands: `detect`, `config`, `init`
- Added GPU detection (NVIDIA via nvidia-smi, Apple Metal)
- Added Jetson and Apple Silicon detection
- Created basic tests for detection module
- Fixed Pydantic deprecation warning (class Config → ConfigDict)

### Files Changed
- `pyproject.toml` — Project configuration and dependencies
- `requirements.txt` — Quick-install dependencies
- `src/__init__.py` — Package init with version
- `src/main.py` — CLI entry point with detect/config/init commands
- `src/core/__init__.py` — Core module exports
- `src/core/detection.py` — System detection (OS, hardware, GPU)
- `src/core/config.py` — Configuration management
- `src/llm/__init__.py` — LLM module placeholder
- `src/memory/__init__.py` — Memory module placeholder
- `src/tools/__init__.py` — Tools module placeholder
- `src/ui/__init__.py` — UI module placeholder
- `tests/__init__.py` — Tests package
- `tests/test_detection.py` — Detection module tests

### Commits
(pending — will commit after checkpoint)

### Findings
- System detected: Windows 10.0.26200, x86_64, NVIDIA RTX 2080 Ti (11GB), CUDA 581.80
- Typer 0.21.1 no longer needs `[all]` extra
- Python 3.13.11 compatible

### Issues
None

### Checkpoint
**Status:** CONTINUE — Phase 1 complete, ready for Phase 2 (Model Layer).

### Next
- Implement Phase 2: ModelProvider abstract base class
- Implement OllamaProvider (localhost:11434)
- Implement `animus pull <model>` command

---